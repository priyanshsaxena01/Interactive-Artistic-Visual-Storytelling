{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil # Module for high-level file operations like removing directory trees\n\n# Define the directory to clean\noutput_directory = \"/kaggle/working/\"\n\nprint(f\"--- Cleaning Output Directory: {output_directory} ---\")\n\n# Check if the directory exists\nif os.path.exists(output_directory) and os.path.isdir(output_directory):\n    items_deleted = 0\n    dirs_deleted = 0\n\n    # List all items (files and directories) in the output directory\n    for item_name in os.listdir(output_directory):\n        item_path = os.path.join(output_directory, item_name)\n\n        try:\n            # Check if it's a file or a symbolic link and remove it\n            if os.path.isfile(item_path) or os.path.islink(item_path):\n                os.remove(item_path)\n                print(f\"Deleted file/link: {item_name}\")\n                items_deleted += 1\n            # Check if it's a directory and remove it recursively\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n                print(f\"Deleted directory and contents: {item_name}\")\n                dirs_deleted += 1\n        except Exception as e:\n            print(f\"Error deleting {item_path}: {e}\")\n\n    print(f\"\\n--- Cleaning Complete ---\")\n    print(f\"Total files/links deleted: {items_deleted}\")\n    print(f\"Total directories deleted: {dirs_deleted}\")\n\nelif not os.path.exists(output_directory):\n    print(f\"Directory '{output_directory}' does not exist. Nothing to clean.\")\nelse:\n     print(f\"'{output_directory}' exists but is not a directory. Cannot clean.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install required libraries\n!pip install --quiet diffusers transformers accelerate safetensors invisible_watermark pillow\n\n# Import necessary libraries for login\nimport os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\n# Retrieve the secret token and login\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n    login(token=hf_token)\n    print(\"Successfully logged into Hugging Face Hub.\")\nexcept Exception as e:\n    print(f\"Error logging into Hugging Face Hub: {e}\")\n    print(\"Please ensure you have added HUGGINGFACE_TOKEN as a secret in Kaggle Add-ons.\")\n\n# Verify GPU availability (optional but good practice)\nimport torch\nif torch.cuda.is_available():\n    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM available: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\nelse:\n    print(\"Warning: No GPU detected. Running on CPU will be very slow.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T12:02:52.312370Z","iopub.execute_input":"2025-04-20T12:02:52.312640Z","iopub.status.idle":"2025-04-20T12:04:07.884020Z","shell.execute_reply.started":"2025-04-20T12:02:52.312622Z","shell.execute_reply":"2025-04-20T12:04:07.883281Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0mm00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully logged into Hugging Face Hub.\nGPU detected: Tesla P100-PCIE-16GB\nVRAM available: 15.89 GB\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# CELL 1: Setup - (Keep as is from the previous corrected version)\n# @title Setup: Install Libraries, Import, Configure APIs & Local Model\n\n# Install necessary libraries\n!pip install google-generativeai diffusers transformers accelerate bitsandbytes Pillow kaggle_secrets --quiet --upgrade\n\nimport os\nimport json\nimport time\nimport random\nimport io\nimport traceback # Import traceback for better error reporting\nfrom pathlib import Path\nfrom IPython.display import display, Image as IPImage, clear_output, Markdown, HTML # Import HTML for grid\nimport torch\nimport google.generativeai as genai\nfrom kaggle_secrets import UserSecretsClient\nfrom diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler, DiffusionPipeline\n\nprint(\"Libraries installed and imported.\")\n\n# --- API Key Setup (Gemini Only) ---\ngoogle_api_key = None; genai_configured = False\ntry:\n    user_secrets = UserSecretsClient(); google_api_key = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n    if google_api_key: genai.configure(api_key=google_api_key); genai_configured = True; print(\"âœ… Google AI Studio API Key configured.\")\n    else: print(\"âŒ Google API Key ('GOOGLE_API_KEY') not found in Kaggle Secrets.\")\nexcept Exception as e: print(f\"âŒ Error accessing Kaggle Secrets or configuring clients: {e}\")\n\n# --- Define Safety Settings for Gemini ---\nsafety_settings = [ {\"category\": c, \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"} for c in [\"HARM_CATEGORY_HARASSMENT\", \"HARM_CATEGORY_HATE_SPEECH\", \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"HARM_CATEGORY_DANGEROUS_CONTENT\"]]\n\n# --- Check GPU ---\nif not torch.cuda.is_available(): print(\"âš ï¸ WARNING: GPU not detected! Local Stable Diffusion requires a GPU.\"); device = \"cpu\"\nelse: gpu_name = torch.cuda.get_device_name(0); print(f\"âœ… GPU detected: {gpu_name}\"); device = \"cuda\"\n\n# --- Initialize Models (Gemini & Local Stable Diffusion) ---\ngemini_model = None; image_pipeline = None\n\ndef get_gemini_model(model_name=\"gemini-1.5-flash-latest\"):\n    global gemini_model\n    if gemini_model is None and genai_configured:\n        try: print(f\"ğŸ¤– Initializing Gemini Model ({model_name})...\"); gemini_model = genai.GenerativeModel(model_name); print(f\"ğŸ¤– Gemini Model ({model_name}) Initialized.\")\n        except Exception as e: print(f\"âŒ Error initializing Gemini model: {e}\"); gemini_model = None\n    elif not genai_configured: print(\"âŒ Cannot initialize Gemini: API key not configured.\")\n    return gemini_model\n\ndef load_image_pipeline(model_id=\"stabilityai/stable-diffusion-2-1-base\"):\n    global image_pipeline\n    if image_pipeline is not None: return image_pipeline\n    if device != \"cuda\": print(\"âŒ Cannot load SD pipeline: No GPU.\"); return None\n    print(f\"â³ Loading SD Pipeline ({model_id}) onto GPU...\")\n    try:\n        pipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, revision=\"fp16\")\n        pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config, use_karras_sigmas=True)\n        pipe = pipe.to(device); image_pipeline = pipe; print(\"ğŸ–¼ï¸ SD Pipeline Loaded.\")\n        return image_pipeline\n    except Exception as e:\n        print(f\"âŒ Error loading SD pipeline: {e}\"); image_pipeline = None\n        if torch.cuda.is_available(): print(\"   Attempting to clear CUDA cache...\"); torch.cuda.empty_cache()\n        return None\n\nget_gemini_model()\nLOCAL_SD_MODEL_ID = \"stabilityai/stable-diffusion-2-1-base\"\nif device == 'cuda': load_image_pipeline(LOCAL_SD_MODEL_ID)\nelse: print(\"Skipping Stable Diffusion load (No GPU).\")\n\nPath(\"story_images\").mkdir(parents=True, exist_ok=True)\nprint(\"Output directory 'story_images' created.\")\nprint(\"\\n--- Setup Complete ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T12:04:07.885320Z","iopub.execute_input":"2025-04-20T12:04:07.885590Z","iopub.status.idle":"2025-04-20T12:04:21.919673Z","shell.execute_reply.started":"2025-04-20T12:04:07.885570Z","shell.execute_reply":"2025-04-20T12:04:21.918974Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement kaggle_secrets (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for kaggle_secrets\u001b[0m\u001b[31m\n\u001b[0mLibraries installed and imported.\nâœ… Google AI Studio API Key configured.\nâœ… GPU detected: Tesla P100-PCIE-16GB\nğŸ¤– Initializing Gemini Model (gemini-1.5-flash-latest)...\nğŸ¤– Gemini Model (gemini-1.5-flash-latest) Initialized.\nâ³ Loading SD Pipeline (stabilityai/stable-diffusion-2-1-base) onto GPU...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model_index.json:   0%|          | 0.00/517 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02b819592d9e43f482ea8788250b7f49"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/diffusers/pipelines/pipeline_loading_utils.py:242: FutureWarning: You are loading the variant fp16 from stabilityai/stable-diffusion-2-1-base via `revision='fp16'` even though you can load it via `variant=`fp16`. Loading model variants via `revision='fp16'` is deprecated and will be removed in diffusers v1. Please use `variant='fp16'` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10ecedcab98c44a08db4f4cbec8e3baf"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/633 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70b8e8ff79984272923fe2cdae7e8de6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1342019d4714475786d77d07772d914e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/460 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12f2d4f6f95746e8910e5a83ee325394"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/976 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a965516ae95d4a79b4389ddc8badd818"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7b011f7513c455aa25d30e6a3db5ae8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/824 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10f1d33b114b4cfa8d41cfbf9810d859"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/681M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91a0da3260da41ceaf4e8e5693ad7ee6"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"scheduler_config.json:   0%|          | 0.00/346 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcc0b3ad249b4143982430ef87152983"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/617 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bddd05a68ce497880d3bf4d0fd63291"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.bin:   0%|          | 0.00/1.73G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dafe058a71c24f8a89e13cc389269337"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.bin:   0%|          | 0.00/167M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a62ef6fcc81840088cea99a56e5c5e63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af8a2bd5c4164190a327852f6e4f1611"}},"metadata":{}},{"name":"stderr","text":"An error occurred while trying to fetch /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1-base/snapshots/1f758383196d38df1dfe523ddb1030f2bfab7741/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1-base/snapshots/1f758383196d38df1dfe523ddb1030f2bfab7741/vae.\nDefaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\nAn error occurred while trying to fetch /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1-base/snapshots/1f758383196d38df1dfe523ddb1030f2bfab7741/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1-base/snapshots/1f758383196d38df1dfe523ddb1030f2bfab7741/unet.\nDefaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ–¼ï¸ SD Pipeline Loaded.\nOutput directory 'story_images' created.\n\n--- Setup Complete ---\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# CELL 2: Core Generation Functions (General Story, Artistic Images) - No Changes Needed from V9\n# @title Core Generation Logic (Gemini & Local SD - General Story, Artistic Images)\n\n# --- Gemini Generation Configs ---\nstory_gen_config = genai.types.GenerationConfig(max_output_tokens=80, temperature=0.75)\nprompt_gen_config = genai.types.GenerationConfig(max_output_tokens=150, temperature=0.6)\n\n# --- Function to generate GENERAL story segment ---\ndef gemini_generate_story_segment(story_history_log):\n    \"\"\"Generates the next short, complete paragraph using Gemini.\"\"\"\n    model = get_gemini_model()\n    if not model: return None, \"Gemini model not available.\"\n    instruction = (\"You are continuing a visual story. Based on the history provided below, write ONLY the next short paragraph (2-3 complete sentences) of the story. Focus on advancing the plot or describing the immediate scene/action. Ensure the paragraph is concise and ends with a complete sentence (no '...'). Be engaging.\\n\\n---\\nHistory:\\n\")\n    history_text = \"\\n\".join([f\"{entry['role']}: {entry['parts'][0]}\" for entry in story_history_log])\n    full_prompt = instruction + history_text\n    print(\"\\nğŸ¤– Asking Gemini for next story segment...\")\n    try:\n        response = model.generate_content(full_prompt, generation_config=story_gen_config, safety_settings=safety_settings)\n        if not response.candidates:\n             feedback = response.prompt_feedback if hasattr(response, 'prompt_feedback') else None; reason = getattr(feedback, 'block_reason', 'Unknown')\n             print(f\"âš ï¸ Gemini story generation blocked. Reason: {reason}\"); return None, f\"Story generation failed (Safety Block: {reason}).\"\n        story_text = response.text.strip()\n        if story_text.endswith(\"...\"): story_text = story_text[:-3].strip()\n        if not story_text: return None, \"Gemini returned an empty story segment.\"\n        print(f\"   Story Segment Generated.\")\n        return story_text, None\n    except Exception as e: print(f\"âŒ Error during Gemini story generation: {e}\"); return None, f\"Error during Gemini story generation: {e}\"\n\n# --- Function to generate a DETAILED image prompt (Specific Art Style) ---\ndef gemini_generate_detailed_image_prompt(story_segment_text, art_style):\n    \"\"\"Generates a DETAILED image prompt focusing on a specific Indian art style.\"\"\"\n    model = get_gemini_model();\n    if not model: return None, \"Gemini model not available for prompt generation.\"\n    instruction = (f\"Based *only* on the following story segment, generate a visually descriptive prompt for an AI image generator. The final image **MUST strictly be in the style of {art_style}**. DO NOT generate a photorealistic image. Focus on artistic representation.\\n\\nInclude elements characteristic of **{art_style}** applied to the scene description. Describe:\\n- Key subjects/characters (**represented artistically in {art_style}**)\\n- Specific actions or poses (**depicted in the conventions of {art_style}**)\\n- Setting details (**rendered in {art_style}**)\\n- Overall mood/atmosphere (**evoking the scene's feeling through the lens of {art_style}**)\\n\\nMake the prompt rich with specific visual keywords related to both the scene and **{art_style}**. Emphasize keywords like '{art_style}', 'painting', 'illustration', 'folk art', 'traditional art'. Strictly avoid terms like 'photorealistic', 'photograph', 'realistic', '3D render', 'CGI'.\\n\\nOutput ONLY the prompt itself, nothing else.\\n\\n---\\nStory Segment:\\n\")\n    full_prompt = instruction + story_segment_text\n    print(f\"\\nğŸ¤– Asking Gemini for detailed image prompt ({art_style})...\")\n    try:\n        response = model.generate_content(full_prompt, generation_config=prompt_gen_config, safety_settings=safety_settings)\n        if not response.candidates:\n             feedback = response.prompt_feedback if hasattr(response, 'prompt_feedback') else None; reason = getattr(feedback, 'block_reason', 'Unknown')\n             print(f\"âš ï¸ Gemini prompt generation blocked. Reason: {reason}\"); fallback_prompt = f\"{story_segment_text[:100]}..., {art_style}, painting, illustration, traditional style\"; return fallback_prompt, f\"Prompt generation failed (Safety Block: {reason}). Using fallback.\"\n        image_prompt = response.text.strip().replace(\"Image Prompt:\", \"\").replace(\"Prompt:\", \"\").strip()\n        if not image_prompt: fallback_prompt = f\"{story_segment_text[:100]}..., {art_style}, painting, illustration, traditional style\"; return fallback_prompt, \"Gemini returned an empty prompt. Using fallback.\"\n        image_prompt = image_prompt.replace(\"photorealistic\", \"\").replace(\"realistic\", \"\").replace(\"photograph\", \"\")\n        if art_style.split(\",\")[0].strip() not in image_prompt: image_prompt = f\"{art_style}, {image_prompt}\"\n        image_prompt += f\", illustration, painting, traditional art style\"\n        print(f\"   Detailed Prompt Generated.\"); return image_prompt, None\n    except Exception as e: print(f\"âŒ Error during Gemini prompt generation: {e}\"); fallback_prompt = f\"{story_segment_text[:100]}..., {art_style}, painting, illustration, traditional style\"; return fallback_prompt, f\"Error during prompt generation: {e}. Using fallback.\"\n\n\n# --- Function to generate artistic image using LOCAL Stable Diffusion ---\ndef generate_visual_asset(prompt, art_style, filename_prefix=\"scene\", neg_prompt=\"photorealistic, photograph, realistic, 3D render, CGI, blurry, deformed, bad anatomy, extra limbs, disfigured, text, signature, watermark, low quality, ugly, poorly drawn\", steps=30, guidance=8.0):\n    \"\"\"Generates an artistic image using the loaded local Stable Diffusion pipeline and saves it.\"\"\"\n    global image_pipeline; pipe = load_image_pipeline(LOCAL_SD_MODEL_ID)\n    if not pipe: return None, \"Stable Diffusion pipeline not available.\"\n    if device != \"cuda\": return None, \"Cannot generate image: No GPU detected.\"\n    # Add randomness to filename prefix for regenerations\n    filename = f\"story_images/{filename_prefix}_{int(time.time())}_{random.randint(1000,9999)}.png\"\n    final_sd_prompt = f\"{art_style}, {prompt}, detailed illustration, painting, vibrant colors\"\n    final_neg_prompt = neg_prompt\n    print(f\"\\nğŸ–¼ï¸ Generating image via Local SD ({LOCAL_SD_MODEL_ID}) in style: {art_style}...\")\n    try:\n        with torch.inference_mode(): image = pipe(prompt=final_sd_prompt, negative_prompt=final_neg_prompt, num_inference_steps=steps, guidance_scale=guidance).images[0]\n        print(\"âœ… Image generated locally.\"); image.save(filename); print(f\"   Image saved to: {filename}\"); return filename, None # Return filename\n    except torch.cuda.OutOfMemoryError:\n        print(\"âŒ GPU Out of Memory Error!\");\n        if torch.cuda.is_available(): print(\"   Attempting to clear CUDA cache...\"); torch.cuda.empty_cache()\n        return None, \"GPU Out of Memory.\"\n    except Exception as e:\n        print(f\"âŒ Error during local image generation: {e}\");\n        if torch.cuda.is_available(): torch.cuda.empty_cache()\n        return None, f\"Error generating image locally: {e}\"\n\nprint(\"Core functions defined.\")\nprint(\"\\n>>> IMPORTANT: Make sure you have run this cell (Cell 2) before running Cell 3! <<<\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T12:04:21.920536Z","iopub.execute_input":"2025-04-20T12:04:21.920791Z","iopub.status.idle":"2025-04-20T12:04:21.936707Z","shell.execute_reply.started":"2025-04-20T12:04:21.920767Z","shell.execute_reply":"2025-04-20T12:04:21.935968Z"}},"outputs":[{"name":"stdout","text":"Core functions defined.\n\n>>> IMPORTANT: Make sure you have run this cell (Cell 2) before running Cell 3! <<<\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# CELL 3: Interactive Story Loop (Simplified, Regen Features, Grid, Save) - V15 End Fix\n# @title â–¶ï¸ Run the Interactive Artistic Visual Storytelling Loop (Regen + Grid + Save)\n\n# --- Define Indian Art Styles ---\nindian_art_styles = [\n    \"Madhubani painting style, traditional Indian folk art\", \"Warli painting style, tribal Indian folk art, geometric patterns\",\n    \"Kalamkari style painting, intricate patterns, natural dyes aesthetic\", \"Gond art style, vibrant patterns, dotted lines, tribal Indian art\",\n    \"Pattachitra style painting, Odisha folk art, mythological themes, bold lines\", \"Mughal miniature painting style, detailed figures, rich borders\",\n    \"Tanjore painting style, gold foil, semi-precious stones aesthetic, South Indian art\", \"Kerala mural painting style, traditional temple art, rich colors, fresco look\",\n]\n\n# --- Initialization ---\nstoryboard_data = []; turn_count = 0; max_turns = 6; selected_art_style = None\npotential_story_text = None; potential_detailed_prompt = None; potential_image_path = None\nregenerating_image = False\nstory_log_for_gen = []\n\n# --- Pre-run Checks ---\nif not genai_configured or not gemini_model: print(\"âŒ Cannot start: Gemini not configured.\")\nelif device != 'cuda':\n     print(\"âš ï¸ WARNING: No GPU detected. Image generation will fail.\"); run_anyway = input(\"Proceed with text only? (yes/no): \").lower()\n     if run_anyway != 'yes': print(\"Exiting.\"); genai_configured = False\nelse: pass\n\n# --- Get Art Style Choice ---\nif genai_configured:\n    clear_output(wait=True); print(\"--- REMINDER: Please ensure you have run Cell 2 AFTER any code changes! ---\")\n    print(\"\\n--- Select Indian Art Style for Images ---\"); [print(f\"{i + 1}: {style.split(',')[0]}\") for i, style in enumerate(indian_art_styles)]\n    while selected_art_style is None:\n        try:\n            choice = int(input(f\"Enter the number (1-{len(indian_art_styles)}): \"));\n            if 1 <= choice <= len(indian_art_styles): selected_art_style = indian_art_styles[choice - 1]; print(f\"\\nâœ… Art style selected: {selected_art_style.split(',')[0]}\")\n            else: print(\"Invalid number.\")\n        except ValueError: print(\"Invalid input.\")\n\n# --- Get Initial Idea & Setup Log ---\nif selected_art_style:\n    print(f\"\\n--- Starting Visual Story (Images in {selected_art_style.split(',')[0]}) ---\")\n    initial_user_idea = input(f\"Enter the starting idea for your story: \")\n    story_log_for_gen = [{\"role\": \"user\", \"parts\": [f\"Start a visual story based on this idea: {initial_user_idea}\"]}]\n\n# --- Main Interactive Loop ---\nwhile genai_configured and selected_art_style and turn_count < max_turns:\n    current_scene_num = turn_count + 1\n    clear_output(wait=True); print(\"--- REMINDER: Please ensure you have run Cell 2 AFTER any code changes! ---\")\n    print(f\"\\n--- Generating Scene {current_scene_num}/{max_turns} (Style: {selected_art_style.split(',')[0]}) ---\")\n    if storyboard_data:\n        print(\"\\n--- Story So Far (Last Accepted Scene) ---\"); last_scene = storyboard_data[-1]\n        display(Markdown(f\"**Scene {len(storyboard_data)} Text:** *{last_scene['text']}*\"))\n        if last_scene['image_path'] and os.path.exists(last_scene['image_path']): display(IPImage(filename=last_scene['image_path'], width=300))\n        print(\"-\" * 20)\n\n    try:\n        # --- 1. Story Generation Step ---\n        story_error_occurred = False\n        if not regenerating_image:\n            temp_story_text, story_err = gemini_generate_story_segment(story_log_for_gen)\n            if story_err: print(f\"\\nâŒ Story Generation Error: {story_err}\"); story_error_occurred = True; potential_story_text = None\n            else: potential_story_text = temp_story_text\n        if potential_story_text: display(Markdown(f\"**Story Draft (Scene {current_scene_num}):** {potential_story_text}\"))\n        elif not story_error_occurred: display(Markdown(\"**[Awaiting Story Generation...]**\"))\n        else: display(Markdown(\"**[Story Generation Failed]**\"))\n\n        # --- 2. Prompt Generation Step ---\n        prompt_error_occurred = False\n        if potential_story_text and not regenerating_image:\n            potential_detailed_prompt, prompt_err = gemini_generate_detailed_image_prompt(potential_story_text, selected_art_style)\n            if prompt_err: print(f\"âš ï¸ Warning (Prompt Gen): {prompt_err}\")\n            print(f\"**Image Prompt Draft:** {potential_detailed_prompt}\")\n        elif regenerating_image and potential_detailed_prompt: print(f\"**Image Prompt (kept):** {potential_detailed_prompt}\")\n        else: potential_detailed_prompt = None\n\n        # --- 3. Image Generation Step ---\n        image_error_occurred = False; potential_image_path = None\n        if device == 'cuda' and potential_detailed_prompt:\n            filename_p = f\"scene_{turn_count}{'_regen' if regenerating_image else ''}\"\n            potential_image_path, image_err = generate_visual_asset(potential_detailed_prompt, selected_art_style, filename_prefix=filename_p)\n            if image_err: print(f\"âŒ Image Generation Failed: {image_err}\"); image_error_occurred = True\n            if potential_image_path: display(IPImage(filename=potential_image_path, width=512))\n            elif not image_error_occurred: display(Markdown(\"**[Awaiting Image Generation...]**\"))\n            else: display(Markdown(\"**[Image Generation Failed]**\"))\n        elif device != 'cuda': print(\"â„¹ï¸ Skipping image generation (No GPU).\")\n        else: print(\"â„¹ï¸ Skipping image generation (No valid prompt).\")\n\n        # --- 4. User Action ---\n        regenerating_image = False; print(\"\\n\" + \"=\"*50)\n        print(f\"Review Scene {current_scene_num}. Action: [C]ontinue | [L]LM Continue | Re[G]en Story | Re[I]m Image | [E]nd\")\n        user_action = input(\"> \").upper().strip()\n\n        # --- Handle User Actions ---\n        if user_action == 'G':\n            if story_error_occurred: print(\"\\nCannot regen story after story gen error.\"); time.sleep(2); continue\n            print(\"\\nğŸ”„ Regenerating story segment...\"); potential_story_text = None; potential_detailed_prompt = None; potential_image_path = None; time.sleep(1); continue\n        elif user_action == 'I':\n            if device != 'cuda': print(\"Cannot regen image without GPU.\"); time.sleep(2); continue\n            if not potential_detailed_prompt: print(\"Cannot regen image without valid prompt.\"); time.sleep(2); continue\n            print(\"\\nğŸ”„ Regenerating image...\"); regenerating_image = True; potential_image_path = None; time.sleep(1); continue\n        # --- MODIFIED END BLOCK ---\n        elif user_action == 'E':\n            print(\"\\nEnding story.\")\n            # --- ADD Check and Append before breaking ---\n            if potential_story_text and (device != 'cuda' or potential_image_path):\n                 # Add the last successfully generated scene if user ends here\n                 print(\"   (Adding last generated scene to storyboard before ending...)\")\n                 storyboard_data.append({\n                     \"text\": potential_story_text,\n                     \"image_path\": potential_image_path, # Will be None if no GPU/image failed\n                     \"prompt\": potential_detailed_prompt,\n                     \"style\": selected_art_style.split(',')[0]\n                 })\n                 print(f\"DEBUG: Storyboard now contains {len(storyboard_data)} scenes after End action.\")\n            else:\n                 print(\"   (Last scene generation was incomplete, not adding to storyboard.)\")\n            # --- End of Add block ---\n            break # Exit the main loop\n        # --- END OF MODIFIED END BLOCK ---\n        elif user_action in ['C', 'L']:\n            # --- ACCEPT TURN ---\n            if not potential_story_text: print(\"\\nCannot continue: Story gen failed.\"); time.sleep(3); continue\n            if device == 'cuda' and not potential_image_path: print(\"\\nCannot continue: Image gen failed.\"); time.sleep(3); continue\n\n            print(f\"\\nâœ… Scene {current_scene_num} Accepted.\")\n            current_story_text = potential_story_text; detailed_prompt = potential_detailed_prompt; image_path = potential_image_path\n            storyboard_data.append({\"text\": current_story_text, \"image_path\": image_path, \"prompt\": detailed_prompt, \"style\": selected_art_style.split(',')[0]})\n            print(f\"DEBUG: Storyboard now contains {len(storyboard_data)} scenes.\")\n            story_log_for_gen.append({\"role\": \"model\", \"parts\": [current_story_text]})\n            turn_count += 1\n            print(f\"DEBUG: Turn count advanced to {turn_count}.\")\n            if turn_count >= max_turns: print(f\"Reached max turns ({max_turns}). Story finished.\"); break\n            print(f\"--- Preparing for Scene {turn_count + 1} ---\")\n            if user_action == 'L': user_input_content = \"Continue the story naturally.\"; print(\"   (AI will continue next turn)\")\n            else: user_input_content = input(\"Enter your idea for the next segment: \").strip(); user_input_content = user_input_content or \"Continue the story naturally.\"\n            story_log_for_gen.append({'role': 'user', 'parts': [user_input_content]})\n            MAX_LOG_ENTRIES = 12;\n            if len(story_log_for_gen) > MAX_LOG_ENTRIES: print(\"--- Pruning story log history ---\"); story_log_for_gen = story_log_for_gen[-MAX_LOG_ENTRIES:]\n            potential_story_text = None; potential_detailed_prompt = None; potential_image_path = None; print(\"-\" * 20); time.sleep(1)\n        else: print(\"Invalid choice. Please try again.\"); time.sleep(2); continue\n    except Exception as loop_err: print(f\"\\nâŒ Unexpected error: {loop_err}\"); traceback.print_exc(); print(\"Exiting loop.\"); break\n\n# --- End of Story / Final Processing ---\nif selected_art_style:\n    print(\"\\n\" + \"=\"*50)\n    if storyboard_data: print(f\"Story finished with {len(storyboard_data)} scenes.\")\n    elif not story_log_for_gen and turn_count == 0: print(\"Story setup failed or was not started.\")\n    else: print(\"Story finished or ended early before any scenes were completed.\")\n\n    # --- Display & Save Final Storyboard in a Grid ---\n    if storyboard_data:\n        print(f\"\\n--- Final Storyboard ({selected_art_style.split(',')[0]}) ---\")\n        print(f\"DEBUG: Generating HTML for {len(storyboard_data)} scenes.\")\n        cols = 2\n        safe_style_name = selected_art_style.split(',')[0].strip().replace(\" \", \"_\").lower()\n        storyboard_filename = f\"storyboard_{safe_style_name}.html\"\n        html_table = f\"<!DOCTYPE html><html><head><title>Storyboard: {selected_art_style.split(',')[0]}</title>\"\n        html_table += \"<style> table {width:100%; border-collapse: collapse;} td {border: 1px solid #ccc; padding: 10px; vertical-align: top; width: \" + str(100/cols) + \"%;} img {max-width: 95%; height: auto; display: block; margin-bottom: 5px;} </style>\"\n        html_table += f\"</head><body><h2>Storyboard: {selected_art_style.split(',')[0]}</h2><table>\"\n        import base64\n        for i, item in enumerate(storyboard_data):\n            if i % cols == 0: html_table += \"<tr>\"\n            cell_html = f\"<td><b>Scene {i+1}</b><br>\"\n            if item['image_path'] and os.path.exists(item['image_path']):\n                 try:\n                     with open(item[\"image_path\"], \"rb\") as img_file: b64_string = base64.b64encode(img_file.read()).decode('utf-8')\n                     cell_html += f'<img src=\"data:image/png;base64,{b64_string}\" alt=\"Scene {i+1}\"><br>'\n                 except Exception as img_err: print(f\"Error encoding image {item['image_path']}: {img_err}\"); cell_html += \"<i>Error loading image</i><br>\"\n            elif device == 'cuda': cell_html += \"<i>Image generation failed or path incorrect</i><br>\"\n            else: cell_html += \"<i>(No image generated - No GPU)</i><br>\"\n            cell_html += f\"<i>{item['text']}</i><br>\"\n            cell_html += f\"<small><i>Style: {item['style']}</i></small><br>\"\n            cell_html += \"</td>\"\n            html_table += cell_html\n            if (i + 1) % cols == 0 or (i + 1) == len(storyboard_data):\n                if (i + 1) == len(storyboard_data) and (i + 1) % cols != 0:\n                    remaining_cols = cols - ((i + 1) % cols); html_table += \"<td style='border: 1px solid #ccc; padding: 10px;'></td>\" * remaining_cols\n                html_table += \"</tr>\"\n        html_table += \"</table></body></html>\"\n        display(HTML(html_table))\n        try:\n            with open(storyboard_filename, \"w\", encoding=\"utf-8\") as f: f.write(html_table)\n            print(f\"\\nâœ… Storyboard saved successfully as: {storyboard_filename}\")\n            print(f\"   Find in Output section ('/kaggle/working/').\")\n        except Exception as e: print(f\"\\nâŒ Error saving storyboard file: {e}\"); traceback.print_exc()\n    else: print(\"\\nNo completed scenes to display in storyboard.\")\n\n# --- Cleanup (Optional) ---\n# del image_pipeline; del gemini_model; image_pipeline = None; gemini_model = None\n# if torch.cuda.is_available(): print(\"\\nClearing CUDA cache...\"); torch.cuda.empty_cache()\n# print(\"\\nModels unloaded and cache cleared (optional).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T12:08:16.584617Z","iopub.execute_input":"2025-04-20T12:08:16.585396Z","iopub.status.idle":"2025-04-20T12:08:16.669788Z","shell.execute_reply.started":"2025-04-20T12:08:16.585366Z","shell.execute_reply":"2025-04-20T12:08:16.668883Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_122/4073760225.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# --- Pre-run Checks ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgenai_configured\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgemini_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"âŒ Cannot start: Gemini not configured.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m      \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"âš ï¸ WARNING: No GPU detected. Image generation will fail.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mrun_anyway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Proceed with text only? (yes/no): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'genai_configured' is not defined"],"ename":"NameError","evalue":"name 'genai_configured' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"# CELL 4: Quantitative Evaluation (Manual CLIP Similarity & Text Coherence) - REVISED\n# @title Calculate Automated Metrics (Run After Cell 3)\n\n# --- Ensure necessary libraries are installed ---\n# transformers and Pillow should be installed by Cell 1\n# sentence-transformers might still need installation if not done previously\n# !pip install sentence-transformers --quiet\n\nimport os\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport torch.nn.functional as F # For cosine similarity\n# Check device again for metric models\nmetric_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device for metrics: {metric_device}\")\n\n# --- 1. Manual CLIP Similarity Calculation ---\nclip_model = None\nclip_processor = None\nclip_similarity_calculated = False\navg_clip_sim_prompt_image = None\navg_clip_sim_text_image = None\n\ntry:\n    from transformers import CLIPProcessor, CLIPModel\n    print(\"\\nLoading CLIP model for similarity calculation (this may take a moment)...\")\n    # Load pre-trained model and processor\n    clip_model_id = \"openai/clip-vit-base-patch32\"\n    clip_processor = CLIPProcessor.from_pretrained(clip_model_id)\n    clip_model = CLIPModel.from_pretrained(clip_model_id).to(metric_device)\n    clip_model.eval() # Set model to evaluation mode\n    print(\"CLIP model loaded.\")\n\n    # --- Function to calculate similarity for a list of image paths and texts ---\n    @torch.no_grad() # Disable gradient calculations for efficiency\n    def calculate_clip_similarity(image_paths, texts):\n        if not image_paths or not texts or len(image_paths) != len(texts):\n            print(\"   Error: Invalid inputs for similarity calculation.\")\n            return None\n\n        similarities = []\n        batch_size = 16 # Process in batches if many images\n        for i in range(0, len(image_paths), batch_size):\n            batch_image_paths = image_paths[i:i+batch_size]\n            batch_texts = texts[i:i+batch_size]\n\n            try:\n                # Load images\n                images = [Image.open(p) for p in batch_image_paths]\n                # Preprocess\n                inputs = clip_processor(text=batch_texts, images=images, return_tensors=\"pt\", padding=True, truncation=True).to(metric_device)\n                # Get embeddings\n                outputs = clip_model(**inputs)\n                image_embeds = outputs.image_embeds\n                text_embeds = outputs.text_embeds\n                # Calculate cosine similarity (using torch.nn.functional)\n                # Normalize embeddings for stable cosine similarity\n                image_embeds = F.normalize(image_embeds, p=2, dim=-1)\n                text_embeds = F.normalize(text_embeds, p=2, dim=-1)\n                # Calculate dot product (equivalent to cosine similarity for normalized vectors)\n                batch_sims = (image_embeds * text_embeds).sum(dim=1) # More numerically stable\n                similarities.extend(batch_sims.cpu().numpy())\n            except Exception as batch_err:\n                 print(f\"   Error processing batch starting at index {i}: {batch_err}\")\n                 # Optionally add None or skip this batch's results\n                 # For simplicity, we'll just skip if a batch fails entirely\n                 pass # Continue to next batch\n\n        if not similarities:\n             print(\"   No similarities could be calculated.\")\n             return None\n\n        return np.mean(similarities) * 100 # Often scaled by 100 in CLIP papers\n\n    print(\"\\nCalculating CLIP Similarities...\")\n    if storyboard_data:\n        # Prepare lists, ensuring images exist and lists align\n        prompts_for_clip = []\n        story_texts_for_clip = []\n        image_paths_for_clip = []\n\n        for i, item in enumerate(storyboard_data):\n            if item.get('image_path') and os.path.exists(item['image_path']) and item.get('prompt') and item.get('text'):\n                 image_paths_for_clip.append(item['image_path'])\n                 prompts_for_clip.append(item['prompt'])\n                 story_texts_for_clip.append(item['text'])\n\n        if not image_paths_for_clip:\n             print(\"   No valid images found in storyboard data.\")\n        else:\n            print(f\"   Calculating similarity for {len(image_paths_for_clip)} valid scenes...\")\n            # Calculate Prompt-Image Similarity\n            avg_clip_sim_prompt_image = calculate_clip_similarity(image_paths_for_clip, prompts_for_clip)\n            if avg_clip_sim_prompt_image is not None:\n                 print(f\"   Average CLIP Similarity (Detailed Prompt vs. Image): {avg_clip_sim_prompt_image:.2f}\")\n                 clip_similarity_calculated = True # Mark as calculated if at least one succeeded\n\n            # Calculate StoryText-Image Similarity\n            avg_clip_sim_text_image = calculate_clip_similarity(image_paths_for_clip, story_texts_for_clip)\n            if avg_clip_sim_text_image is not None:\n                 print(f\"   Average CLIP Similarity (Story Text vs. Image):    {avg_clip_sim_text_image:.2f}\")\n                 clip_similarity_calculated = True # Mark as calculated\n            elif avg_clip_sim_prompt_image is None: # If both failed\n                 clip_similarity_calculated = False\n\n\n    else:\n        print(\"   Storyboard data is empty. Cannot calculate CLIP similarity.\")\n\nexcept ImportError:\n    print(\"\\n'transformers' library not found. Skipping CLIP similarity calculation.\")\n    print(\"   It should be installed by Cell 1. If not, run: pip install transformers\")\nexcept Exception as e:\n    print(f\"\\nError during CLIP similarity calculation: {e}\")\n    import traceback\n    traceback.print_exc()\nfinally:\n    # --- Clean up CLIP model from memory ---\n    del clip_model\n    del clip_processor\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    print(\"CLIP model unloaded.\")\n\n\n# --- 2. Text Coherence (Sentence-BERT) ---\n# (Keep this section as is)\ncoherence_calculated = False\navg_coherence = None\nstd_coherence = None\ntry:\n    from sentence_transformers import SentenceTransformer, util\n    print(\"\\nCalculating Text Coherence...\")\n\n    if len(storyboard_data) > 1:\n        print(f\"   Using {len(storyboard_data)} story segments...\")\n        sbert_model = SentenceTransformer('all-MiniLM-L6-v2', device=metric_device)\n        story_texts = [item['text'] for item in storyboard_data]\n        embeddings = sbert_model.encode(story_texts, convert_to_tensor=True, device=metric_device)\n        similarities = []\n        for i in range(len(embeddings) - 1):\n            emb1 = embeddings[i].to(metric_device)\n            emb2 = embeddings[i+1].to(metric_device)\n            sim = util.pytorch_cos_sim(emb1, emb2)\n            similarities.append(sim.item())\n\n        if similarities:\n            avg_coherence = np.mean(similarities)\n            std_coherence = np.std(similarities)\n            print(f\"   Average Consecutive Segment Similarity: {avg_coherence:.4f}\")\n            print(f\"   Std Dev Consecutive Segment Similarity:  {std_coherence:.4f}\")\n            coherence_calculated = True\n        else:\n             print(\"   Not enough valid similarities calculated.\")\n    else:\n        print(\"   Need at least 2 story segments for coherence calculation.\")\n\nexcept ImportError:\n    print(\"\\nsentence-transformers library not found. Skipping coherence calculation.\")\n    print(\"   Install using: pip install sentence-transformers\")\nexcept Exception as e:\n    print(f\"\\nError calculating text coherence: {e}\")\n    import traceback\n    traceback.print_exc()\n\n\n# --- Final Summary ---\nprint(\"\\n\" + \"=\"*30)\nprint(\"--- Evaluation Summary ---\")\nprint(\"=\"*30)\nif clip_similarity_calculated:\n    # Check if values were actually calculated before printing\n    if avg_clip_sim_prompt_image is not None:\n        print(f\"Avg. CLIP Sim (Prompt-Image): {avg_clip_sim_prompt_image:.2f}\")\n    else:\n        print(\"Avg. CLIP Sim (Prompt-Image): Error during calculation.\")\n    if avg_clip_sim_text_image is not None:\n        print(f\"Avg. CLIP Sim (Text-Image):   {avg_clip_sim_text_image:.2f}\")\n    else:\n        print(\"Avg. CLIP Sim (Text-Image):   Error during calculation.\")\nelse:\n    print(\"CLIP Similarity: Not calculated (or failed).\")\nprint(\"-\" * 30)\nif coherence_calculated:\n    print(f\"Avg. Text Coherence:      {avg_coherence:.4f} (StdDev: {std_coherence:.4f})\")\nelse:\n    print(\"Text Coherence: Not calculated.\")\nprint(\"=\"*30)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-20T12:08:03.514Z"}},"outputs":[],"execution_count":null}]}